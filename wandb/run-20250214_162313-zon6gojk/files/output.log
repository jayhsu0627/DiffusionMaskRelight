INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 985
INFO:__main__:  Num Epochs = 100
INFO:__main__:  Instantaneous batch size per device = 1
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 16
INFO:__main__:  Gradient Accumulation steps = 16
INFO:__main__:  Total optimization steps = 6200
Steps:   2%|████▎                                                                                                                                                                               | 147/6200 [31:55<21:40:02, 12.89s/it, lr=8.82e-6, step_loss=0.877]Traceback (most recent call last):
VAE precision: torch.float16
UNet precision: torch.float32
Trainable VAE precision: torch.float32
Optimizer dtype: torch.float32
pixel_values dtype: torch.float32
  File "/sdb5/DiffusionMaskRelight/train_svd_relight.py", line 1472, in <module>
    main()
  File "/sdb5/DiffusionMaskRelight/train_svd_relight.py", line 1297, in main
    model_pred = unet(
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/accelerate/utils/operations.py", line 819, in forward
    return model_forward(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/accelerate/utils/operations.py", line 807, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/sdb5/DiffusionMaskRelight/models/unet_spatio_temporal_condition.py", line 482, in forward
    sample = upsample_block(
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/diffusers/models/unets/unet_3d_blocks.py", line 1529, in forward
    hidden_states = attn(
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/diffusers/models/transformers/transformer_temporal.py", line 361, in forward
    hidden_states_mix = temporal_block(
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/diffusers/models/attention.py", line 749, in forward
    attn_output = self.attn2(norm_hidden_states, encoder_hidden_states=encoder_hidden_states)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/diffusers/models/attention_processor.py", line 588, in forward
    return self.processor(
  File "/opt/conda/envs/DiffLight/lib/python3.10/site-packages/diffusers/models/attention_processor.py", line 3290, in __call__
    hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
KeyboardInterrupt
