Using LOCAL_RANK=0 on GPU 0,1
/var/spool/slurm/slurmd.spool/job3664387/slurm_script: line 27: export: `12340': not a valid identifier
NODELIST=gammagpu15
MASTER_ADDR=gammagpu15
Traceback (most recent call last):
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/bin/accelerate", line 10, in <module>
    sys.exit(main())
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1163, in launch_command
    multi_gpu_launcher(args)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/accelerate/commands/launch.py", line 792, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 864, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 683, in _initialize_workers
    self._rendezvous(worker_group)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 500, in _rendezvous
    rdzv_info = spec.rdzv_handler.next_rendezvous()
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 67, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. port: 29500, useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use
srun: error: gammagpu15: task 1: Exited with exit code 1
Using local rank: -1 (from env)Using local rank: -1 (from env)

WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
INFO:__main__:Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: fp16

INFO:__main__:Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 2
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: fp16

Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loading model from: /fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
Loading model from: /fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
modify_unet_input in progress: 24 8
after modification 28
after modification 3 3
modify_unet_input in progress: 24 8
after modification 28
after modification 3 3
Loaded 1 preprocessed samples.
Loaded 1 preprocessed samples.
gammagpu15:254593:254593 [0] NCCL INFO Bootstrap : Using bond0:192.168.44.26<0>
gammagpu15:254593:254593 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
gammagpu15:254593:254593 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
gammagpu15:254593:254593 [0] NCCL INFO NET/Plugin: Using internal network plugin.
gammagpu15:254593:254593 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.21.5+cuda12.4
gammagpu15:254593:254593 [0] NCCL INFO Comm config Blocking set to 1
gammagpu15:254594:254594 [1] NCCL INFO cudaDriverVersion 12040
gammagpu15:254594:254594 [1] NCCL INFO Bootstrap : Using bond0:192.168.44.26<0>
gammagpu15:254594:254594 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
gammagpu15:254594:254594 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
gammagpu15:254594:254594 [1] NCCL INFO NET/Plugin: Using internal network plugin.
gammagpu15:254594:254594 [1] NCCL INFO Comm config Blocking set to 1
gammagpu15:254593:254649 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
gammagpu15:254593:254649 [0] NCCL INFO NET/Socket : Using [0]bond0:192.168.44.26<0> [1]virbr0:192.168.122.1<0>
gammagpu15:254593:254649 [0] NCCL INFO Using non-device net plugin version 0
gammagpu15:254593:254649 [0] NCCL INFO Using network Socket
gammagpu15:254594:254650 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
gammagpu15:254594:254650 [1] NCCL INFO NET/Socket : Using [0]bond0:192.168.44.26<0> [1]virbr0:192.168.122.1<0>
gammagpu15:254594:254650 [1] NCCL INFO Using non-device net plugin version 0
gammagpu15:254594:254650 [1] NCCL INFO Using network Socket
gammagpu15:254593:254649 [0] NCCL INFO ncclCommInitRank comm 0x55aa29bafb80 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 81000 commId 0x8ea199820065a2b6 - Init START
gammagpu15:254594:254650 [1] NCCL INFO ncclCommInitRank comm 0x55e2ecf65440 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId c1000 commId 0x8ea199820065a2b6 - Init START
gammagpu15:254593:254649 [0] NCCL INFO comm 0x55aa29bafb80 rank 0 nRanks 2 nNodes 1 localRanks 2 localRank 0 MNNVL 0
gammagpu15:254594:254650 [1] NCCL INFO comm 0x55e2ecf65440 rank 1 nRanks 2 nNodes 1 localRanks 2 localRank 1 MNNVL 0
gammagpu15:254593:254649 [0] NCCL INFO Channel 00/04 :    0   1
gammagpu15:254594:254650 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1 [2] -1/-1/-1->1->0 [3] 0/-1/-1->1->-1
gammagpu15:254593:254649 [0] NCCL INFO Channel 01/04 :    0   1
gammagpu15:254593:254649 [0] NCCL INFO Channel 02/04 :    0   1
gammagpu15:254594:254650 [1] NCCL INFO P2P Chunksize set to 131072
gammagpu15:254593:254649 [0] NCCL INFO Channel 03/04 :    0   1
gammagpu15:254593:254649 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1 [2] 1/-1/-1->0->-1 [3] -1/-1/-1->0->1
gammagpu15:254593:254649 [0] NCCL INFO P2P Chunksize set to 131072
gammagpu15:254594:254650 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
gammagpu15:254593:254649 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
gammagpu15:254594:254650 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
gammagpu15:254593:254649 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
gammagpu15:254594:254650 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
gammagpu15:254593:254649 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
gammagpu15:254594:254650 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
gammagpu15:254593:254649 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
gammagpu15:254594:254650 [1] NCCL INFO Connected all rings
gammagpu15:254594:254650 [1] NCCL INFO Connected all trees
gammagpu15:254593:254649 [0] NCCL INFO Connected all rings
gammagpu15:254594:254650 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
gammagpu15:254593:254649 [0] NCCL INFO Connected all trees
gammagpu15:254594:254650 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gammagpu15:254593:254649 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
gammagpu15:254593:254649 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
gammagpu15:254594:254650 [1] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
gammagpu15:254593:254649 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
gammagpu15:254593:254649 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
gammagpu15:254594:254650 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
gammagpu15:254593:254649 [0] NCCL INFO ncclCommInitRank comm 0x55aa29bafb80 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 81000 commId 0x8ea199820065a2b6 - Init COMPLETE
gammagpu15:254594:254650 [1] NCCL INFO ncclCommInitRank comm 0x55e2ecf65440 rank 1 nranks 2 cudaDev 1 nvmlDev 1 busId c1000 commId 0x8ea199820065a2b6 - Init COMPLETE
VAE precision: torch.float16
UNet precision: torch.float32
Optimizer dtype: torch.float32
wandb: Currently logged in as: sjxu (sjxu_gamma) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: Run data is saved locally in /fs/nexus-scratch/sjxu/DiffusionMaskRelight/wandb/run-20250226_014259-9ifk4jpj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-cosmos-127
wandb: ‚≠êÔ∏è View project at https://wandb.ai/sjxu_gamma/DiffSVD_Relight
wandb: üöÄ View run at https://wandb.ai/sjxu_gamma/DiffSVD_Relight/runs/9ifk4jpj
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 1
INFO:__main__:  Num Epochs = 4000
INFO:__main__:  Instantaneous batch size per device = 2
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 32
INFO:__main__:  Gradient Accumulation steps = 8
INFO:__main__:  Total optimization steps = 4000
VAE precision: torch.float16
UNet precision: torch.float32
Optimizer dtype: torch.float32
pixel_values dtype: torch.float16
pixel_values dtype: torch.float16
  0%|          | 0/4000 [00:00<?, ?it/s]Steps:   0%|          | 0/4000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/fs/nexus-scratch/sjxu/DiffusionMaskRelight/train_svd_relight_syn.py", line 1572, in <module>
    main()
  File "/fs/nexus-scratch/sjxu/DiffusionMaskRelight/train_svd_relight_syn.py", line 1355, in main
    encoder_hidden_states = torch.where(
RuntimeError: The size of tensor a (2) must match the size of tensor b (24) at non-singleton dimension 0
[rank1]: Traceback (most recent call last):
[rank1]:   File "/fs/nexus-scratch/sjxu/DiffusionMaskRelight/train_svd_relight_syn.py", line 1572, in <module>
[rank1]:     main()
[rank1]:   File "/fs/nexus-scratch/sjxu/DiffusionMaskRelight/train_svd_relight_syn.py", line 1355, in main
[rank1]:     encoder_hidden_states = torch.where(
[rank1]: RuntimeError: The size of tensor a (2) must match the size of tensor b (24) at non-singleton dimension 0
[rank0]: Traceback (most recent call last):
[rank0]:   File "/fs/nexus-scratch/sjxu/DiffusionMaskRelight/train_svd_relight_syn.py", line 1572, in <module>
[rank0]:     main()
[rank0]:   File "/fs/nexus-scratch/sjxu/DiffusionMaskRelight/train_svd_relight_syn.py", line 1355, in main
[rank0]:     encoder_hidden_states = torch.where(
[rank0]: RuntimeError: The size of tensor a (2) must match the size of tensor b (24) at non-singleton dimension 0
gammagpu15:254594:254651 [1] NCCL INFO [Service thread] Connection closed by localRank 1
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mlogical-cosmos-127[0m at: [34mhttps://wandb.ai/sjxu_gamma/DiffSVD_Relight/runs/9ifk4jpj[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250226_014259-9ifk4jpj/logs[0m
gammagpu15:254594:254961 [1] NCCL INFO comm 0x55e2ecf65440 rank 1 nranks 2 cudaDev 1 busId c1000 - Abort COMPLETE
gammagpu15:254593:254652 [0] NCCL INFO [Service thread] Connection closed by localRank 0
[rank0]:[W226 01:43:06.817495277 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
gammagpu15:254593:254963 [0] NCCL INFO comm 0x55aa29bafb80 rank 0 nranks 2 cudaDev 0 busId 81000 - Abort COMPLETE
W0226 01:43:07.092000 254576 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 254593 closing signal SIGTERM
E0226 01:43:07.307000 254576 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 254594) of binary: /fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/bin/python3.10
Traceback (most recent call last):
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/bin/accelerate", line 10, in <module>
    sys.exit(main())
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1163, in launch_command
    multi_gpu_launcher(args)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/accelerate/commands/launch.py", line 792, in multi_gpu_launcher
    distrib_run.run(args)
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/fs/nexus-scratch/sjxu/miniconda3/envs/DiffLight/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_svd_relight_syn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-26_01:43:07
  host      : gammagpu15.umiacs.umd.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 254594)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: gammagpu15: task 0: Exited with exit code 1
